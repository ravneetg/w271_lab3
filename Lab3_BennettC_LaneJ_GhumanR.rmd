---
title: "W271 - Section 1 Lab 3"
author: "Chris Bennett, Jackson Lane, Ravneet Ghuman"
date: "November 10, 2016"
output: html_document
---

##W271 - Section 1 Lab 3

##Predicting Income From 2014 American Community Survey Response Data

###Table of Contents

* Project Description
    * Data Science Question
    * Project Overview
    * About the Chosen Methodology - OLS Linear Regression
    * About The 2014 American Community Survey
* Exploratory Data Analysis
    * Observations On The Entire Data Set 
    * Evaluation of Candidate Dependent Variables
    * Evaluation of Candidate Independent Variables
* Data Transformation
* EDA of Transformed Variables
* Model Development
* Model Analysis
    * Model Performance Evaluation
    * Assumptions and Diagnostics
    * Assumption Violation Remediation
* Model Selection
* Conclusion - How Does The Model Help Answer The Data Science Question

##Project Description

###Data Science Question

How accurately can one predict a person's income by utilizing the ACS survey data, such as occupation industry, hours worked, commute time, ancestry, language, location, age, education level and marital status?

Why we chose this question: In the marketing industry, knowing an individual's income is important for determining whether or not they will be likely to purchase an item (or service) of a specific amount. In many cases, consumers are willing to provide details around many aspects of their lives to surveys and warrantee forms, but they are less likely to provide their personal income.  We are approaching this survey to see if we can predict income by using potentially more readily available data (e.g. language spoken, education, commute time, work hours, et al).

###Overview

We utilized a portion of the 2014 American Community Survey data, as collected by the US. Census Bureau to conduct an OLS Multi-variate Linear Regression Analysis of survey data to predict a person's income (as self-reported in the same ACS survey data).

The American Community Survey (ACS) is an annual survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year. Through the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. Public officials, planners, and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. This ACS Overview paragraph was borrowed from the ACS website (http://www2.census.gov/programs-surveys/acs)

The American Community Survey (ACS) is administered, processed, researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce.

###About the Chosen Methodology - OLS Linear Regresson

In order to answer the question "How accurately can one predict an individual's annual income using survey response information provided by citizens in the American Community Survey", we selected a classic analytics workhorse - the OLS Multiple Linear Regression approach. This approach allows for the prediction of a dependent variable by multiple independent variables similar to other machine learning approaches (i.e. K Nearest Neighbor, Naive Bayes, Random Forest, et al), but the OLS regression approach allows us the ability to not only assess the accuracy of the model, but to also thoroughly analyze the underlying statistical assumptions that the model is based on.

###About the 2014 American Community Survey

Prepared by American Community Survey Office - U.S. Census Bureau - October 27, 2015. The data used in this lab is referred to as the Public Use Microdata Sample (PUMS). The Public Use Microdata Sample (PUMS) contains a sample of actual responses to the American Community Survey (ACS). Each record in the file represents a single person, or--in the household-level dataset--a single housing unit. PUMS files for an individual year, such as 2014, contain data on approximately one percent of the United States population. PUMS files covering a five-year period, such as 2010-2014, contain data on approximately five percent of the United States population. 

Since all ACS responses are strictly confidential, many variables in the PUMS files have been modified in order to protect the confidentiality of survey respondents. For instance, particularly high incomes are "top-coded," uncommon birthplace or ancestry responses are grouped into broader categories, and the PUMS files provide a very limited set of geographic variables (explained more below).

We will explore the top-coding of the income variable more in the following sections of this paper.

####Weights in PUMS

The ACS PUMS is a weighted sample, and weighting variables must be used to generate accurate estimates and standard errors. The PUMS files include both population weights and household weights. Population weights should be used to generate statistics about individuals, and household weights should be used to generate statistics about housing units. The weighting variables are described briefly below.

The analysis conducted in this paper excludes the use of any weighting, which does introduce potential bias into the exogeneity of the model results, but helps to simplify the data analysis for the purposes of this Lab Exercise.

The 2014 ACS Survey Data Dictionary can be found here: http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.pdf

The actual 2014 Survey questions can be found at:
http://www2.census.gov/programs-surveys/acs/methodology/questionnaires/2014/quest14.pdf

Some additional information on the survey's accuracy can be found at: http://www2.census.gov/programs-surveys/acs/tech_docs/pums/accuracy/2015AccuracyPUMS.pdf

All technical documentation for the ACS data can be found at:
http://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.html

```{r}

###################################################################################################################
#                                                                                                                 #
# Load All Necessary Libraries                                                                                    #
#                                                                                                                 #
###################################################################################################################

library(psych)
library(dplyr)
library(plyr)
library(corrplot)
library(reshape2)
require(lmtest)
library(gvlma)
library(reshape2)
library(gvlma)
library(car)
library(Hmisc)
library(dummy)
library(lattice)
library(ggplot2)
library(corrplot)
dev.off()
rm(list=ls())

#setwd("/Users/rghuman/Documents/ds/w271/assignments/lab3/w271_lab3")

#load RData file
load("w271_lab3.Rproj.RData")


##################################################################################################################
# Code to read data from input files included in case RData file not available
# The following block of code loads the two survey files and concatenates them together
# ----------------------------------------------------------------------------------------------------------------
# mydata = read.csv("ss14pusa.csv", header=TRUE)
# mydatab = read.csv("ss14pusb.csv", header=TRUE)
# 
# names(mydata)
# names(mydatab)
# 
# acsdata = rbind(mydata,mydatab)
# 
# nrow(mydata) + nrow(mydatab)
# nrow(acsdata)
# ncol(mydata)
# 
# ls(acsdata)
# describe(acsdata)  #list the variables and the labels for each
# str(acsdata)       #list the variables, datatypes, and first few observations for each
# names(acsdata)
# 
# summary(acsdata)
##################################################################################################################

```

###Data features used for this study:
ADJINC -- Adjustment factor for income and earnings dollar amounts. The value of ADJINC inflation-adjusts reported income to 2015 dollars and applies to FINCP and HINCP in the housing record.

```{r}

##################################################################################################################
#                                                                                                                #
# Exploratory Data Analysis Helper Functions                                                                     #
#                                                                                                                #
##################################################################################################################

#OutlierKD script borrowed from Klodian Dhana at https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
#To detect the outliers I use the command boxplot.stats()$out which use the Tukey's method to identify the outliers ranged above and below the 1.5*IQR. To describe the data I preferred to show the number (%) of outliers and the mean of the outliers in dataset. I also show the mean of data with and without outliers. Regarding the plot, I think that boxplot and histogram are the best for presenting the outliers. In the script below, I will plot the data with and without the outliers. Finally, with help from Selva, I added a question (yes/no) to ask whether to keep or remove the outliers in data. If the answer is yes then outliers will be replaced with NA.

outlierKD <- function(dt, var_name,label) {
     na1 <- sum(is.na(var_name))
     m1 <- mean(var_name, na.rm = T)
     par(mfrow=c(2, 2), oma=c(0,0,3,0))
     boxplot(var_name, main="With outliers")
     hist(var_name, main="With outliers", xlab=NA, ylab=NA)
     outlier <- boxplot.stats(var_name)$out
     mo <- mean(outlier)
     var_name <- ifelse(var_name %in% outlier, NA, var_name)
     boxplot(var_name, main="Without outliers")
     hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
     title(paste("Outlier Check",label), outer=TRUE)
     na2 <- sum(is.na(var_name))
     cat("Outliers identified:", na2 - na1, "n")
     cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "n")
     cat("Mean of the outliers:", round(mo, 2), "n")
     m2 <- mean(var_name, na.rm = T)
     cat("Mean without removing outliers:", round(m1, 2), "n")
     cat("Mean if we remove outliers:", round(m2, 2), "n")
}

#The following function was split out from the preceding fuction.  Attribution for the code is given above.
outlierRM <- function(dt, var) {
     response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
     if(response == "y" | response == "yes"){
          dt[as.character(substitute(var))] <- invisible(var_name)
          assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
          cat("Outliers successfully removed", "n")
          return(invisible(dt))
     } else{
          cat("Nothing changed", "n")
          return(invisible(var_name))
     }
}

# Function to make a frequency table
# Works better once variables are changed to named factors
makeFreqTable = function(label){
w = table(acs[[label]])
x = ddply(acs,label,summarise,mean=mean(PINCP,na.rm=TRUE),sd=sd(PINCP,na.rm=TRUE))
x = x[!is.na(x[[label]]),]
t = as.data.frame(w)
tt = as.data.frame(x)
names(t)[1] = label
names(t)[2] = 'Freq'
ttt <- cbind(t,tt[,-1])
j <- cbind(ttt,round(ttt$Freq/length(acs[[label]])*100,2))
names(j)[5] = 'Perc'
return(head(j[order(-j$Perc),],20))
}

#Creates a scatterplot of variable versus PINCP
#Pull a random sample and scatterplot to see if there are differences in income by category

makeScatterPlot = function(label){
acs2 <- acs[sample(1:nrow(acs), 50000, replace=FALSE),c(label,"PINCP")]
series = acs2[[label]]
scatterplot( series,acs2$PINCP, main=paste("Scatterplot of PINCP over",label),xlab=label,ylab="PINCP")
}

#Generic EDA function.
runEDA = function(label){
  series = acs[[label]]
print("Summary:")
print(summary(series))
print("Describe:")
print(describe(series))
print("Head:")
print(head(series,20))
descriptiveStats = t(data.frame("Number of zero values"=sum(series == 0,na.rm=T),
                          "Number of NA values" = sum(is.na(series)),
                          "Number of non-zero, non-NA values" = sum(series > 0,na.rm = T),
                          "Number of Unique values" = length(unique(series))
                          ))
if(is.numeric(series)){
outlierKD(acs, series,label)
  }

descriptiveStats

}
```

##Exploratory Data Analysis:

###Overall Dataset

The original data set was comprised of two files that had the same number of features. These files were combined into a single file through a simple file append (rbind).  The combined file contains 3,132,610 records and 284 variables. Our project team methodically went through each variable and identified those that conceptually provide some relationship to predicting income. Given more time and resources, a more thorough analysis of all 284 variables might have netted a few additional variables that helped improve our regression model.  However, we narrowed the working variable set from 284 to 27 variables that we believed might hold some potential to predicting income (e.g. Years of School Completed, Age, et al). The 27 variables that were kept were divided into 3 categories: potential dependent variables (e.g. income, wages, etc.), potential independent variables, and meta data (e.g. unique record identifiers, weighted values, etc.).

###Evaluate Possible Dependent Variables

We evaluated three different variables that are related to a person's income to determine which of the three would be have the highest likelihood of performing well as a Y-variable in a linear regression.  Overall, we evaluated the range/spread, distribution, top & bottom-coding, and the number of unique values. We also conducted some evaluation on correlation with the different dependent variables under consideration from the ACS study.

NOTE: It is suggested in the ACS Survey Documentation that we use ADJINC to adjust PERNP, PINCP, and WAGP to constant dollars at 2014 value. We have chosen not to make this adjustment for the sake of this analysis, as their relative values to the independent variables is our need in developing the most accurate and predictive value possible. Additionally, this adjustment factor has the same value for all records, which further indicates it will have no impact (positive or negative) on our model development.

####1.) PERNP - Total Person's Earnings

Description of this field: Total Person's Earnings

The following description is from the data dictionary:
bbbbbbb          .N/A (less than 15 years old)
0000000          .No earnings
-010000          .Loss of $10000 or more (Rounded & bottom-coded .components)
-000001..-009999 .Loss $1 to $9999 (Rounded components)
0000001          .$1 or break even
0000002..9999999 .$2 to $9999999 (Rounded & top-coded components)

Our analysis of the PERNP variable:

* Missing values (NAs): 581,723
* Mean: 28,300
* Median: 10,000
* Min: -8,500
* Max: 1,030,000
* Zero values: 1,539,488
* Number of Unique Values: 5,060
* Number of non-NA, non-zero values: 2,550,887
* Distribution: Non-normal distribution with a very positive skew

####2.) PINCP - Total person's income (signed)

Description of this field: Total person's income (signed)

The following description is from the data dictionary:
bbbbbbb          .N/A (less than 15 years old)
0000000          .None
-019999          .Loss of $19999 or more (Rounded & bottom-coded components)
-000001..-019998 .Loss $1 to $19998 (Rounded components)
0000001          .$1 or break even
0000002..9999999 .$2 to $9999999 (Rounded & top-coded components)

Our analysis of the PINCP variable:

* Missing values (NAs): 541,687
* Mean: 36,500
* Median: 21,500
* Min: -13,000
* Max: 1,400,000
* Zero values: 881,976
* Number of Unique Values: 19,622
* Number of non-NA, non-zero values: 2,590,923 <- Double check this...
* Distribution: Non-normal with a very positive skew

####3.) WAGP - Wages or Salary Income Past 12 Months

Description of this field: Wages or Salary Income Past 12 Months

The following description is from the data dictionary:
bbbbbb          .N/A (less than 15 years old)
000000          .None
000001..999999  .$1 to 999999 (Rounded and top-coded)

Our analysis of the WAGP variable:

* Missing values (NAs): 541,687
* Mean: 26,100
* Median: 6,000
* Min: 0
* Max: 642,000
* Zero values: 1,636,095
* Number of Unique Values: 912
* Number of non-NA, non-zero values: 2,590,923
* Distribution: Non-normal with a very positive skew

####Dependent Variable Conclusion:

Upon inspection, PINCP has the fewest number of zero values, is tied for the fewest NA's, and has by far the most unique values. Given these characteristics, PINCP would likely make the most sense as the Dependent Variable for this model.  Therefore, for the remainder of this lab, we focused on predicting the PINCP from a number of the predictor variables that will be discussed further on. 

Also, due to similarities of WAGP and PINCP, it appears that one of the variables was likely generated from the other.  The attributes of these two variables do have some slight differences, but their similarities are strong enough to be noted.

#####Possible Transformations for PINCP:

Because the distribution of PINCP is so concentrated on lower values and has a strong positive skew, we will evaluate models utilizing both a log transform of PINCP, as well as the raw value of PINCP.  Also, since we can only predict values with PINCP, we will reduce the dataset to only include records with a non-NA value in the PINCP variable. This will give us a maximum dataset size of 2,590,923.  Additional transformations will be discussed further into this paper.

###Evaluate Possible Independent Variables

####4.) SERIALNO - Unique ID (Meta Data)

This variable is the housing unit or person serial number, and serves primarily as a unique identifier.  There are no missing values and 1.36 million distinct value. The reason that there are not unique/distinct values for every record is because if multiple people in one household complete the survey, they each are assigned the same SERIALNO, but each given unique SPORDER (Person numbers). The SerialNo field is not useful for a regression model, but is useful for data quality diagnostics.

####5.) SPORDER - Person Number (Meta Data)

The SPORDER variable is combined with the SERIALNO variable to create a single unique variable for all records in teh full 3.1 million record data set. The SPORDER variable is not useful for a regression model, but is useful for data quality diagnostics.

####6.) ST - State Code

The State Code did not contain any missing data.  The data type is a 2-byte integer, categorical variable, that refers to a corresponding State (or district) as outlined in the data dictionary. There were 51 distinct values of this variable (not including N/A), with the most frequently occuring being 06 (California). The data quality appears to be strong and there are no issues with "catch-all values", or top/bottom coding. This field was considered for model development.

####7.) POWSP - Place of work: State or Foreign Country Code

The Place of Work field contained 59 distinct values (not including N/A) and 1.74 million missing values. The data type was a 3-byte integer, categorical variable, that has a corresponding city/state or country listed in a referrence table in teh data dictionary. If the value is blank, as is the case for 1.74mm records, that represents several possible answers which include: military, unenployed, under 16, does not work in the US, et al. Due to the large number of missing values, and the multitude of possible meanings of a N/A, we decided to not pursue this variable in our model development.

####8.) ADJINC - Adjusted Income factor

The value of ADJINC inflation-adjusts reported income to 2014 dollars. ADJINC applies to variables FINCP and HINCP in the housing record, and variables INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, and WAGP in the person record. The value of the ADJINC variable is the same for all records, "1008425", and for the primary purpose of weighting several numerical values to increase their exogeneity for the US population, and for comparing values across years. Since it was decided by our group that we would not be adjusting variables using the weighting value in this field, the ADJINC field was not utilized further in this project.

####9.) AGEP - Age of person

The AGEP variable contains no missing values and 97 distinct values. There are 31,366 records with zero as the value of this variable, which the data dictionary explains represents ages under 1 year. The maximum value is 96. The data dictionary explains that this variable is potentially top-coded, so we carefully examined the frequency of the maximum values. Because there were not many records with the value 96, we deemed any top-coding to be insignificant, or possibly even non-existent. The median age of this sample population was 42, with a mean of 40.82. This variable appeared to be a good candidate for our regression models. 

####10.) QTRBIR - Quarter of birth

Quarter of Birth was kept on the dataset for possible use as an Instrumental Variable, if needed to identify/control for Omitted Variable Bias. This integer variable had no missing values and was fairly evenly distributed between 1, 2, 3, and 4.

####11.) SEX - Male or Female

The SEX variable is a 1-byte integer field where 1 equals Male, and 2 equals Female. There are no NA or zero values. There are 1.52 million males and 1.60 females represented.

####12.) MAR - Marital Status

The Marital Status Variable is a 1-byte integer, categorical variable, with the following values outlined in the data dictionary:

1 .Married
2 .Widowed
3 .Divorced
4 .Separated
5 .Never married or under 15 years old

The MAR variable has no NA or zero values. The Married (1) and Never Married or Under 15 (5) values have approximately 1.3 million record each (2.62 million combined). Divorced (3) was in third place with 277,870 records.

####13.) MARHT - Number of times married

The MARHT (number of times married) field contains 1,304,577 NA's. This variable is a 1-byte integer that includes 1, 2, and 3 as possible values. The majority of records that didn't have an NA, were coded with a 1 (1.36 million). There were 361,174 records with a 2, and 104,413 records with a 3. This variable doesn't show much potential for a regression model because of the lack of values and the sparse number of categories. 

####14.) MARHYP - Year Last Married

The MARHYP (year last married) field contains a 4-byte integer that represents the year the individual was married. The MARHYP variable contains a blank if the individual's age is less than 15, or they were never married. There were 1,304,577 n/a values and 83 distinct values. The most frequent value was 2000. This variable may have a spurious correlation with age, number of times married, or married.

####15.) SCHL - Years of Schooling

The SCHL (years of school) variable, or Educational Attainment is comprised of a 2-byte integer ordinal value that ties back to a referrence table listed in the data dictionary. If the individual had less than 3 years of school, or they are less than 3 years old, the SCHL value was N/A.  There were 96,124 N/A's in this dataset. The most frequent value is 16, which represents a regular high-school diploma. The second most frequent is 21, which represents the completion of a bachelor's degree. The maximum value was 23, which represented a professional degree beyond a bachelor's degree.  The referrence table has a value for 24 (doctorate), but there were none of these in the data. There were 25 distinct values present. Due to the obvious potential relationship between number of years of education and income, this is an ideal candidate for an independent variable in our model.

####16.) FOD1P - Field of degree - first entry

The FOD1P Variable is a 4-byte integer categorical variable with 174 unique values present in the data. There are 2.44 million N/A's in this variable, with only 695,652 populated with a valud value. An N/A could represent no response, or an education less than a bachelor's degree. The most frequent value (6203) Business Management and Administration had 43,135 records. The next most frequent was Elementary Education with 30,738.

####17.) FCITP - Citizenship Allocation Flag

The FCITP variable (Citizenship Allocation Flag) is a binary variable where 0 represents Not a Citizen, and 1 represents that the individual is a citizen. 2.95 million records (or 94.17%) had the value zero. There were no N/a values.

####18.) JWMNP - Travel Time to Work

The JWMNP variable (Travel Time to Work) is a 3-byte interval variable that represents the number of minutes to get to work. The data dictionary states that this variable is potentially top-coded. The maximum value (164) is infrequent, and represents an outlier which should be removed for regression analysis. The mean value is 26.2 minutes, with a median of 20.0 minutes. 25.4% of the non-N/A commute times are for a commute time of 10 minutes or less, 56% are for 20 minutes or less, and 76.1% are for 30 minutes or less. There are 1,807,678 missing values, which can represent someone who is not a worker, or someone who works at home. There are 144 distinct values for this variable in the dataset. Besides the potential top-coding, and the multiple meanings of n/a values, the continuous values with a good range of values makes it an interesting variable that will be tested further for the regression model.

####19.) WKHP - Usual Hours Worked Per Week Past 12 Months

The WKHP variable (Usual Hours Worked Per Week) variable is a 2 byte integer that reflects either the number of hours worked per week, n/a for less than 16 years old or didn't work during the past 12 months, or 99 for 99 or more usual hours. There are 1.54 million missing values in this variable and the value with the highest frequency ws 40 hours (663,449), with 50 hours coming in second place (131,188), and 45 hours in 3rd (86,792). The minimum value (not including N/A) is 8 hours (13,319) and the maximum is 60 hours (58,296). Because of the intuitive potential relationship between hours worked per week and income, this variable is an ideal candidate for our regression model.

####20.) YOEP - Years of entry into the US (For individuals not born in the US)

The YOEP variable (Year of entry into the US) is a 4-byte integer data type with the year that represents with the survey participant entered the US. There are 2.73 million missing values, which represent "not eligible - Born in the US". There are 85 unique values. The Mean value is 1991, the median is 1994. The max value is 2014 and the min value is 1921. The most common value is 2000 with a frequency of 15,164.  The second most frequent value is 1999, and third is 2001. This variable is sparse as it only applies to individuals not born in the US. This variable also has a possible relationship/correlation with age, language, and ancestry.  Also, this variable is bottom coded. People who entered before 1920 are still coded as 1920.   

####21.) INDP: Industry Code

The INDP (Industry Code) variable represents the code that the individual is employed in. There are 1.29 million N/A's, which represent that the individual is either less than 16 or that they have not worked in at least 5 years. This variable is a 4-byte categorical variable with 268 distinct industry codes.

The top 3 industries are:

  *7860 .EDU-ELEMENTARY AND SECONDARY SCHOOLS (117,774)
  *0770 .CON-CONSTRUCTION, INCL CLEANING DURING AND IMM AFTER (110,735)
  *8680 .ENT-RESTAURANTS AND OTHER FOOD SERVICES (106,776)

####22.) NAICSP: NAICS Industry code

The NAICS Industry code variable has a one-to-one correspondance with the previously described INDP variable. There are 268 distinct NAICS codes, and they are assigned consistent with the INDP codes. For the remainder of this paper, we will utilize the INDP Industry Code instead of the NAICSP as we build our model.

The top NAICSP codes are:

  *6111 .EDU-ELEMENTARY AND SECONDARY SCHOOLS
  *23 .CON-CONSTRUCTION, INCL CLEANING DURING AND IMM AFTER

####23.) ANC1P: First-reported Ancestry

The ANC1P (First-reported Ancestry) variable is a 3-byte categorical variable that has 467,439 records that are "Not Reported" (999), these should coded to NA's if we use this feature. There are 27 zero values - although there is no coded value for zero. These should be removed if we use this feature. There are 39,969 values which code to "Uncodable Entries". These should also be removed if we use this feature. There are 0 values coded as N/A.  There are 231 unique values. 

The top ANC1P values are:

* 999 - Not Reported (14.9%)
* 032 - German (11%)
* 902 - African American (6.8%)
* 50  - Irish (6.7%)
* 939 - American (6.4%)
* 22  - English (5.9%)
* 210 - Mexican (5.8%)
* 51  - Italian (4.3%)
* 924 - White (3.4%).
* 142 - Polish (2%)
* 26  - French (1.5%)
* 996 - Uncodable Entries (1.3%) *Should be Corrected to NA's
* 88  - Scottish (1.2%)
* 706 - Chinese (1.1%)
* 82  - Norwegian ( 1.1%)
* 195 - European (1%) *We might want to recode all European's?
* 920 - American Indian (1%)

####24.) LANP - Language Spoken At Home

The LANP (Language Spoken at Home) variable is a 3-byte integer categorical variable. The coded values represent a list of different languages.  The N/A values represent less than 5 years old/speaks only english. 2,594,174 records are missing a value for the LANP Variable. This variable may not be a great variable because only 17% have values other than N/A.  There are 110 distinct values in this variable. Additionally, the fact that they have combined two scenarios (under 5 y/o and english only) into the N/A value is problematic for the problem we are trying to solve.

Top results by frequency of language type

* 625 - Spanish (10%)
* 708 - Chinese (0.64%)
* 742 - Tagalog (0.54%)
* 728 - Vietnamese (0.43%)
* 620 - French (0.36%)
* 607 - German (0.34%)
* 724 - Korean (0.32%)

####25.) NATIVITY - Native or Foreign Born

The NATIVITY Variable (Native or Foreign Born) is a categorical variable that indicates either that the individual is native, or foreign born. All records have a value, no NA's present. 2,771,933 records are Native (88%), and 360,677 records are Foreign Born (12%). The mean and standard deviation of PINCP (income) for the two values is very close, which raises questions about how useful this variable will be in predicting income.

####26.) POBP - Place of Birth

The POBP (Place of Birth) variable is a 3-byte integer data type categorical variable. There are no N/A values, but several coded values are "catch-alls" (i.e. 399 = Americas, not specified). There appears to be good distribution of values with 215 unique values for this variable, which makes it look like a good candidate variable for the regression model.

Top returned Places of Birth include:

* 006  - California (8.5%)
* 036 - New York (6.7%)
* 048 - Texas (6.1%)
* 042 - Pennsylvania (4.6%)
* 017 - Illinois (4.3%)
* 039 - Ohio (4.1%)
* 026 - Michigan (3.4%)
* 303 - Mexico (3%)

```{r}
######################################################################################################
#                                                                                                    #
# Perform Exploratory Data Analysis on all Variables                                                 #
#                                                                                                    #
######################################################################################################

#EDA for entire data set
ls(acs)
names(acs)
describe(acs)
summary(acs)
nrow(acs) #total rows
ncol(acs) #total cols
nrow(complete.cases(acs))

#PERNP: total person's earnings
runEDA("PERNP")

#PINCP: Total person's income
runEDA("PINCP") 

#WAGP: Total person wages/salary
runEDA("WAGP")

#review SERIALNO: Unique ID
describe(acs$SERIALNO)

#SPORDER: person number
runEDA("SPORDER")

#ST: state
runEDA("ST")
makeFreqTable("ST")
makeScatterPlot("ST")

# POWSP: Place of work - State or foreign country recode
runEDA("POWSP")
makeFreqTable("POWSP")
makeScatterPlot("POWSP")

# Percentage of people working in the US
(table(acs$POWSP<=56)[2]/sum(table(acs$POWSP<=56)))*100
# Since there could be other confounding variables that influence income
# for outside US employments, so we should exclude those records

#ADJINC: Adjusted Income factor
head(acs$ADJINC)
runEDA("ADJINC")

#AGEP: Age of person
runEDA("AGEP")
makeFreqTable("AGEP")
makeScatterPlot("AGEP")

#QTRBIR: Quarter of birth
runEDA("QTRBIR")
makeFreqTable("QTRBIR")
makeScatterPlot("QTRBIR")

#SEX: Male or Female
runEDA("SEX")
makeFreqTable("SEX")
makeScatterPlot("SEX")

#MAR: Marital status
runEDA("MAR")
makeFreqTable("MAR")
makeScatterPlot("MAR")

#MARHT: Number of times married
runEDA("MARHT")
makeFreqTable("MARHT")
makeScatterPlot("MARHT")

#MARHYP: Years married
runEDA("MARHYP")
makeFreqTable("MARHYP")
makeScatterPlot("MARHYP")

#SCHL: Years of Schooling
runEDA("SCHL")
makeFreqTable("SCHL")
makeScatterPlot("SCHL")

#FOD1P: Field of degree - first entry
runEDA("FOD1P")
makeFreqTable("FOD1P")
makeScatterPlot("FOD1P")

#FCITP: Citizenship allocation flag
runEDA("FCITP")
makeFreqTable("FCITP")
makeScatterPlot("FCITP")

#JWMNP: travel time to work
runEDA("JWMNP")
makeFreqTable("JWMNP")
makeScatterPlot("JWMNP")
# Percentage of people with commute less than 10 mins
(table(acs$JWMNP<=10)[2]/sum(table(acs$JWMNP<=10)))*100
# Percentage of people with commute less than 20 mins
(table(acs$JWMNP<=20)[2]/sum(table(acs$JWMNP<=20)))*100
# Percentage of people with commute less than 30 mins
(table(acs$JWMNP<=30)[2]/sum(table(acs$JWMNP<=30)))*100

#WKHP: Usual hours worked per week past 12 months
runEDA("WKHP")
makeFreqTable("WKHP")
makeScatterPlot("WKHP")
# Percentage of people working 20 hrs or less
(table(acs$WKHP<=20)[2]/sum(table(acs$WKHP<=20)))*100
# Percentage of people working 40 hrs
(table(acs$WKHP==40)[2]/sum(table(acs$WKHP==40)))*100
# Percentage of people working more than 40 hrs
(table(acs$WKHP>40)[2]/sum(table(acs$WKHP>40)))*100

#YOEP: Years of entry
runEDA("YOEP")
makeFreqTable("YOEP")
makeScatterPlot("YOEP")

#INDP: industry recode
runEDA("INDP")
makeFreqTable("INDP")
makeScatterPlot("INDP")

#NAICSP: NAICS Indstry code
runEDA("NAICSP")

#ANC1P: First-reported Ancestry
runEDA("ANC1P")
makeFreqTable("ANC1P")
makeScatterPlot("ANC1P")

#LANP: Language Spoken at Home
runEDA("LANP")
makeFreqTable("LANP")
makeScatterPlot("LANP")
j = makeFreqTable("LANP")
#Sort Language by mean income
head(j[order(-j$mean),],20)

#NATIVITY: Native or Foreign Born
runEDA("NATIVITY")
makeFreqTable("NATIVITY")
makeScatterPlot("NATIVITY")

runEDA("POBP")
makeFreqTable("POBP")
makeScatterPlot("POBP")
#Sort by mean income
j = makeFreqTable("POBP")
head(j[order(-j$mean),],20)

```

##Evaluating Correlations Across the Variables

Utilizing the cor (and corrplot) functions, we analyzed the correlations between the variables. The strongest positive correlation was between LANP (Language Spoken at Home) and Ancestry, with a score of 0.507. This correlation is intuitive due to the relationship of one's ancestry and what language they would speak at home. In a similar vein, there is also a positive correlation between the Citizenship Allocation Flag (FCITP) and the Ancestry variable, with a score of 0.43. A positive correlation exists between Income (PINCP) and Work Hours Per Week (WKHP) with a score of 0.3. A negative correlation was evident with the AGEP and the MAR (marrital status) variables, with a score of -0.45.

```{r}
#########################################################################################################
#                                                                                                       #
# Evaluate Correlations Between Variables                                                               #
#                                                                                                       #
#########################################################################################################

#Correlation among variables - to identify relationships that warrant further investigation
#acssmall <- acs[sample(1:nrow(acs), 2000000, replace=FALSE),]

myvars <- names(acs) %in% c("SERIALNO","ST","SPORDER","AGEP","SEX", "MAR", "SCHL", "FCITP", "JWMNP", "WKHP", "INDP", "ANC1P", "LANP", "POBP", "PINCP")

par(mfrow=c(1,1))
M <- cor(acs[myvars], use="complete")
corrplot(M, method="circle")
M

```

#Variable Transformations:

After performing EDA on the raw dataset, we decided on the following actions:

* PINCP: Dependent variable.  Transform with Log
* POWSP: Independent variable.  Transform to binary - US/Non-US
* POBP: Independent variable.  Transform to binary - US/Non-US
* AGP: Independent variable.  Bucket by decade
* SEX: Independent variable. 
* MAR: Independent variable.  Transform to categorical
* SCHL: Independent variable  Bucket by (No schooling, Some Highschool, GED, Associates Degree, Bachelors Degree, Masters Degree or higher)
* INDP: Independent variable  Transform to categorical
* ANC1P: Independent variable  Transform to categorical
* LANP: Independent variable  Bucket by (English, Spanish, Other buckets)
* FCITP: Independent variable
* JWMNP: Independent variable
* WKHP: Independent variable

##


##Filters
This section applies filters on
a) records that have an income of 0 or NA
b) records that have age <15 (as by default younger than 15 have no reported income in the dataset)#rg

```{r}
acs <- acs[acs$PINCP > 0,]
acs <- acs[!is.na(acs$PINCP),]
acs = acs[acs$AGEP>=15,]#rg
```

```{r}

########################################
#Data Transformation for PINCP         #
########################################


#acsTransformed <- data.frame(PINCP_log = log(acs$PINCP))
acs$PINCP_log <- log(acs$PINCP)

#Select a sample of the population to test everything on
#Note: Recode all transformation steps against full dataset once finalized
par(mfrow=c(1,1))
hist(acs$PINCP_log)

########################################
#Data Transformation for AGEP          #
########################################
acs$AGEP_bin = acs$AGEP
hist(acs$AGEP_bin)
summary(acs$AGEP_bin)
describe(acs$AGEP_bin)
length(na.omit(acs$AGEP_bin != 0))

# set starting age as 15 as records with age 14 or lower were filtered out
acs$AGEP_bin <- cut(acs$AGEP, c(15, 20, 30, 40, 50, 60, 70, 80, 90, 100))#rg

table(acs$AGEP_bin)

###MAR####
acs$MAR_bin = factor(acs$MAR)#rg
levels(acs$MAR_bin) = c("Married","Widowed","Divorced","Separated","Not Married")#rg


###########################################################
#Data Transformation for SCHL (Years of School Completed) #
###########################################################

par(mfrow=c(1,1))
acs$SCHL_bin <- as.numeric(cut(acs$SCHL, c(-1, 13, 16, 20, 22, 24))) #rg
table(acs$SCHL_bin)

ggplot(acs, aes(x=SCHL_bin)) + xlim(0,7) + geom_bar(stat="count", fill="#0072B2", colour="black") + ggtitle("Years of School")

##############################################
#Data Transformation for FCITP (Citizenship) #
##############################################

acs$FCITP_bin= acs$FCITP
summary(acs$FCITP_bin)
length(na.omit(acs$FCITP_bin != 0))
hist(acs$FCITP_bin)

######################################################
#Data Transformation for JWMNP (Travel Time to Work) #
######################################################

acs$JWMNP_bin= acs$JWMNP
acs$JWMNP_bin[is.na(acs$JWMNP)] = 0
summary(acs$JWMNP_bin)
length(na.omit(acs$JWMNP_bin != 0))
length(acs$JWMNP_bin[acs$JWMNP_bin==0])
hist(acs$JWMNP_bin)

######################################################
#Data Transformation for WKHP (Hours Worked per Week) #
######################################################
acs$WKHP_bin= acs$WKHP
acs$WKHP_bin[is.na(acs$WKHP)] = 0
summary(acs$WKHP_bin)
length(na.omit(acs$WKHP_bin != 0))
hist(acs$WKHP_bin)

######################################################
#Data Transformation for INDP:  #
######################################################
acs$INDP_bin = factor(acs$INDP)#rg
acs$INDP_bin=addNA(acs$INDP_bin) #rg

summary(acsTransformed$INDP)
plot(acsTransformed$INDP)


######################################################
#Data Transformation for LANP (Language Spoken)      #
######################################################
acs$LANP_bin="Non-English, Non-Spanish" #rg

acs$LANP_bin[is.na(acs$LANP)]<- "English" #rg
acs$LANP_bin[acs$LANP == 625] <- "Spanish" #rg
acs$LANP_bin = factor(acs$LANP_bin) #rg

######################################################
#Data Transformation for ANC1P (Ancestry)            #
######################################################

acs$ANC1P_bin = factor(acs$ANC1P) #rg
acs$ANC1P_bin = addNA(acs$ANC1P_bin)#rg

length(is.null(acs$ANC1P_bin))
summary(acs$ANC1P_bin)

######################################################
#Data Transformation for POBP (Place of Birth)       #
######################################################

acs$POBP_US_bin = acs$POBP < 72 #changed from 73 to 72 #rg

######################################################
#Data Transformation for POWSP (Place of Work)       #
######################################################

acs$POWSP_US_bin = acs$POWSP < 72 #changed from 73 to 72 #rg
# filter so that place of work is in US 
# (as there could be other confounding variables that affect international income)

```


##Model training

##Split into training and test data
```{r}


acs.testdata=acs[sample(nrow(acs),2220000,replace=FALSE), ]
acs.small.testdata=acs.testdata[sample(nrow(acs.testdata), 30,replace=FALSE), ]
acs.trainingdata = anti_join(acs, acs.testdata, by =c("SERIALNO","SPORDER"))

# test to make sure test data + training data = full dataset
nrow(acs.trainingdata)+nrow(acs.testdata)==nrow(acs)

# acspull <- acssmall[sample(1:nrow(acs), 500000, replace=FALSE),]#rg
# 
# head(acssmall)#rg
# nrow(acssmall)#rg
# 
# acspull <- acssmall[sample(1:nrow(acssmall)), ]#rg
# 
# acsmod <- head(acspull,140000)#rg
# acstest <- tail(acspull,5000)#rg

modtest = lm(PINCP_log ~ AGEP_bin + SCHL_bin + 
               JWMNP_bin + WKHP_bin + JWMNP_bin*WKHP_bin,
             data=acs.trainingdata)#rg
summary(modtest)

plot(modtest)

#predictions = predict(lm(log(PINCP) ~ factor(AGEP_bin) + factor(SCHL_bin) + JWMNP + WKHP + 
#factor(INDP), acstest)) #rg

#### code for predictions
predictions = predict.lm(modtest,acs.small.testdata,se.fit = FALSE, interval = "confidence" )#rg

### view side by side
head(cbind(predictions,log(acs.testdata$PINCP)))#rg

acs.testdata$fit=as.vector(predictions[,1])#rg
acs.testdata$lwr=as.vector(predictions[,2])#rg
acs.testdata$upr=as.vector(predictions[,3])#rg
acs.testdata$lPINCP=log(acs.testdata$PINCP)#rg

plot.data=(acs.testdata[c("lPINCP","fit","lwr","upr")])#rg
plot.data=plot.data[complete.cases(plot.data),]#rg
plot(log(plot.data$PINCP), type = "l",col="red")#rg
lines(plot.data$fit,col="green")#rg
head(plot.data)#rg

#lines(plot.data$upr,col="blue")
#lines(plot.data$lwr,col="blue")

(meanDiff <- mean(log(acstest$PINCP) - predictions))

predictions = predict(lm(log(PINCP) ~ WKHP, acstest))
(meanDiff <- mean(log(acstest$PINCP) - predictions))

```

##Model Assumptions and Diagnostics

```{r}

#Helper function to check if a regression model is statistically significant.
checkModelSig = function(model){
  
  f = summary(model)$fstatistic
p = pf(f[1],f[2],f[3],lower.tail = F)
return(p<.05)
}


#Function to check model assumptions, plot residual charts
runDiagnostics = function(model){

# The Shapiro-Wilk test of normality can only handle 5000 data points.
# Take a random sample of residuals if there are more than 5000 data points
if(nrow(model$model) < 5000)  {
    residualsSubset = model$residuals
}else{
    residualsSubset = sample(model$residuals,5000)

}

  
# At least for my computer, R is not able to plot models with too many training points.  
# So only draw plots for models with less than 50000 data points (number choosen arbitrarily)
# We can still test the OLS assumptions though with statistical tests 
if(nrow(model$model) < 50000){
par(mfrow=c(1,2))
hist(model$residuals, main="Histogram of Model Residuals")
qqnorm(model$residuals)
qqline(model$residuals)
scatterplot(model$fitted.values, model$residuals,
            smoother = loessLine, cex = 0.5, pch = 19,
            smoother.args = list(lty = 1, lwd = 5),main = "Actual Vs Fitted Values")
residualPlots(model, main="Plots of Residuals vs Indepenent Variables")

}
  
# Formal statistical tests of OLS assumptions
statTests = as.data.frame(rbind(c("Linearity","Rainbow test",raintest(model)$p.value > .05),c("Normality of Residuals","Shapiro test",shapiro.test(residualsSubset)$p.value > .05),c(  "Homoskedasticity","Breusch-Pagan test",
bptest(model)$p.value > .05),c("Autocorrelation","Durbin-Watson test",durbinWatsonTest(model)$p > .05)))
names(statTests) = c("Assumption","Test","Passed")

# Exogeneity tests for each independent variable
exoTests = sapply(model$model[-1],function(x){!checkModelSig(lm(model$residuals~x-1))})
names(exoTests) = names(model$model[-1])
exoTests = data.frame(Assumption = paste("Exogeneity for",names(exoTests)),Test="Regress residuals on variable",Passed=exoTests, row.names=NULL)

# Return combined table
rbind(statTests,exoTests)
}
```


###Overall Observations on Statistical Validity of the Model

The survey applies adjustments for several variables collected in the survey to weight the data in order to better resemble a nationally representative sample. This means that the ACS data is fundamentally biased, and may not be portrayed as fully accurate, exogenous tool. The weight values were present in the data set for some of the variables, but were not used for this analysis.

While some variables were reported as being top or bottom-coded, our analysis showed that the number of impacted records was very small, and deemed to be not significantly impactful to the model results as their frequency was minimal.

And while many income values in survey data are categorical, the ACS treats this variable as an ordinal feature, making it much more conduscive for OLS regression analysis.

###1.) Random Sampling

From the Census Bureau literature: (https://www.census.gov/content/dam/Census/programs-surveys/acs/about/ACS_Information_Guide.pdf)

The Census Bureau selects a random sample of addresses to be included in the ACS. Each address has about a 1-in-480 chance of being selected in a month, and no address should be selected more than once every 5 years. The Census Bureau mails questionnaires to approximately 295,000 addresses a month across the United States. This is a small number of households considering there are more than 180 million addresses in the United States and an address that receives ACS instructions will not likely fi nd a neighbor or friend who has also received them.

Observations:

The Census Bureau randomly selects its list of addresses from the US Postal Service, which is known for being quite complete and accurate. However, error and omissions do exists, specifically (but not limited to) new homes, seasonal dwellings, and for transient or homeless individuals.  But note that to address this last source of error, the Census Bureau does send surveys to group homes. Another source of error is that this data is obviously only available for compliant sample subjects who complete the information completely and accurately.  While this assumption is helped by the fact that this survey is mandated by law, it is possible that compliance rates may be lower for certain communities.

###Linearity in Parameters

[The model violates the assumption of linearity in parameters as can be seen in the plot of the residuals against the fitted values. The remediate for this is to look at transformations for the contributing parameters (AGE) and to include additional interaction terms for ...]

###No Perfect Collinearity

Explained as, in the sample (and therefore in the population), the assumption is that none of the independent variables are constant and there are no exact relationships among the independent variables. While this assumption only rules out perfect collinearity/correlation between explanatory variables, imperfect correlation is allowed. In practice, high correlation can greatly increase errors.

Observations:

[The following explanatory variables appear to have high correlation... Constant variables are also ruled out, as they are collinear with the intercept term. To remediate this, the following variables  will be eliminated. ] 

###Mean of error term is zero (zero conditional mean)

Explained as, even if we look at a specific value of x, we still expect errors to average to zero. The explanatory variable must not contain information about the mean of ANY unobserved factors. This isn't a strong assumption, because we could always change $$ \beta_0  $$ to move our line up or down so that the mean error is zero. If the errors had a common nonzero mean, and you fitted a least square model, it would be absorbed by the constant, and the residuals would on average be zero. So you can't test whether the residuals have a common mean that's not zero. What you can check is whether the residuals (and by implication the errors that they estimate) have constant mean; on average they're still zero, but conditionally they may have means some distance from zero.

Observations:

The Residuals Vs. Fitted plot as a loess smooth superimposed, but without the curve you can still discern that ... [i.e. the points tend to sit above the zero line at each end and perhaps below it in the middle. You can immediately see that the linearity assumption is somewhat suspect, and that perhaps some curved relationship is present.]

[By utilizing a log transformation of the dependent variable, PINP, we are able to show a more linear relationship betweent the predictors and the dependent variable.]

[If we can't meet the Assumption of Zero Conditional Mean, we might be able to meet the weaker assumption of exogeneity.]

###Omitted Variable Bias

This model violoates the zero conditional mean assumption with respect to omitted variable bias because of the lack of several features that are likely represented in the error term, including: wealth and education of parents, what their lifestyle costs (thrifty?), or how they choose to live or spend their money, income in previous years (that may have provided adequate funds for subsequent years), and other living expenses...as well as many others. This indicates that the model's estimates could be biased due to omitting several potentially important variables.

###Exogeneity

Because of the large size of our dataset, exogeneity is a critical assumption. We tested that the explanatory variables are uncorrelated with the error term (called exogenity). If $$ \x_j $$ is exogenous, $$ Cov(\x_j,u)=0 $$

###Homoskedasticity - Variance of error term is constant

The error term can't vary more for some values of our predictor variables than others, otherwise stated as the values of the explanatory variables must contain no information about the variability of the error. This is a strong assumption.

[The null hypothesis of homoskedasticity specified in the *Breusch-Pagan* test is rejected.]

[The QQ plot shows some evidence of heteroskedasticity. The distribution of the residuals was nearly normal with some positive skew, further indicating possible heteroskedasticity. The residual vs. fitted plot confirms this by showing non-even distribution of variance.]

[Remediation for Heteroskedasticity is to utilize heteroskedasticity-robust tools such as the White standard errors.]

[The QQ plot shows some evidence of heteroskedasticity. The distribution of the residuals was nearly normal with some positive skew, further indicating possible heteroskedasticity.]

###Normality of error term:

[Due to the (very) large sample size, small deviations from the assumption of a normal distribution of error terms do not necessarily negate the statistical significance of tests that require this assumption. Our model does show significant deviations in the normality of the error term, which we attempted to reduce the effects by using robust tools (i.e F-test), as well as by transforming the [XXX] variables.]

###Other sources of bias or error:

The data is also collected at a person and household level. When a survey reaches a household, each person living in the household is asked to complete the form. It is likely that there may be more similarities than differences among individuals living together. Also, we chose the income person's income as the dependent variable because household income was not readily available for analysis.  This could introduce some bias in that certain people are likely benefiting from their partner's income, therefore their lifestyle-related survey responses may not be reflective of their personal income.

```{r}

#PLACE INITIAL MODELS HERE
mod0 <- lm(acs)
mod1 <- 
mod2 <- 

#Utilize White Standard Error because of evidence of Heteroskedasticity
mod0_robust <- coeftest(model0, vcov = vcovHC)
mod1_robust <- coeftest(model1, vcov = vcovHC)
mod2_robust <- coeftest(model2, vcov = vcovHC)

stargazer(mod0_robust, mod1_robust, mod2_robust, type = "text")

#Evaluate if there is any difference between the different models we generated
waldtest(mod0, mod1)
waldtest(mod1, mod2)

#Pick best model and run majority of diagnostics on that model (which will be named mod0)

```

```{r}

runDiagnostics(mod0)

# #Model Assumption Diagnostics Plots
# plot(mod0)
# residualPlots(mod0)
# 
# #Evaluate serial correlation of the residuals
# plot(mod0$residuals, main="Autocorrelation Function of Model Residuals")
# acf(0$residuals, main="Autocorrelation Function of Model Residuals")
# 
# #Correlation among variables - to identify relationships that warrant further investigation
# library(corrplot)
# M <- cor(acs)
# corrplot(M, method="circle")
# 
# shapiro.test(mod0$residuals)
# 
# #Breusch-Pagan Test for Homoskedasticity
# bptest(mod0)
# 
# par(mfrow=c(1,1))
# hist(mod0$residuals)
# qqnorm(mod0$residuals)
# qqline(mod0$residuals)
# scatterplot(mod0$fitted.values, mod0$residuals, smoother = loessLine, cex = 0.5, pch = 19,    smoother.args = list(lty = 1, lwd = 5))
# 
# #Exogeneity
# exo_mod1 <- lm(mod0$residuals ~ acs$AGEP)
# exo_mod2 <- lm(mod0$residuals ~ acs$VAR2)
# exo_mod3 <- lm(mod0$residuals ~ acs$VAR3)
# exo_mod4 <- lm(mod0$residuals ~ acs$VAR4)
# exo_mod5 <- lm(mod0$residuals ~ acs$VAR5)
# 
# summary(exo_mod1)
# summary(exo_mod2)
# summary(exo_mod3)
# summary(exo_mod4)
# summary(exo_mod5)
```


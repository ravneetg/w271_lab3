---
title: "Applied Regression and Time Series Analysis: Lab 2"
author: "Chris Bennett, Jackson Lane, Ravneet Ghuman"
date: "December 15, 2016"
output: pdf_document
---

##Instructions:

*  **Due Date: 12/16/2016 (Friday)**
*  Late submission will not receive any credit.
*  Instructions must be followed strictly.
*  This lab can be completed in a group of up to 3 people. Each group only needs to make one submission.
*  Submit 2 files: (1) a report (in pdf format) detailing your analyses; (2) your R script or juypter notebook supporting all of your answers. Missing one of this files will result in an automatic 50% reduction in score.
*  Use only techniques and R libraries that are covered in this course.
*  If you use R libraries and/or functions to conduct hypothesis tests not covered in this course, you will have to explain why the function you use is appropriate for the hypothesis you are asked to test
*  Thoroughly analyze the given dataset or data series. Detect any anomalies in each of the variables.
*  Your report needs to include a comprehensive graphical analysis
*  Your analysis needs to be accompanied by detailed narrative. Just printing a bunch of graphs and econometric results will likely receive a very low score.
*  Your analysis needs to show that your models are valid (in statistical sense).
*  Your rationale of using certian metrics to choose models need to be provided. Explain the validity / pros / cons of the metric you use to choose your "best" model.
*  Your rationale of any decisions made in your modeling needs to be explained and supported with empirical evidence.
*  All the steps to arrive at your final model need to be shown and explained clearly.
*  All of the assumptions of your final model need to be thoroughly tested, explained, and shown to be valid. Don't just write something like, *"the plot looks reasonable", or "the plot looks good*, as different people interpret vague terms like "reasonable" or "good" differently.
*  Students are expected to act with regards to UC Berkeley Academic Integrity.


# Forecast Inflation-Adjusted Gas Price
During 2013 amid high gas prices, the Associated Press (AP) published an article about the U.S. inflation-adjusted price of gasoline and U.S. oil production. The article claims that there is "*evidence of no statistical correlation*" between oil production and gas prices. The data was not made publicly available, but comparable data was created using data from the Energy Information Administration. The workspace and data frame *gasOil.Rdata* contains the U.S. oil production (in millions of barrels of oil) and the inflation-adjusted average gas prices (in dollars) over the date range the article indicates.

You have three tasks for this exericse, and both tasks need the use of the data set *gasOil.Rdata*.

```{r}
library(dtplyr)
library(Hmisc)
library(astsa)
library(stargazer)
library(forecast)
library(readr)
library(reshape2)
library(car)
library(vars)
library(tseries)
library("GGally")

#setwd("/Users/rghuman/Documents/DS/w271/assignments/w271_2016Fall_lab2")
load("gasOil.Rdata")
```


Task 1: Create a **SARIMA-type** model to forecast the inflation-adjusted gas prices, and use this model to forecast the inflation-adjusted gas price for the next two years.

##Exploratory Data Analysis

###Overall Observations

* There are 410 rows of data, and 3 columns
* The 3 columns are Date, Production, and Price
* There appear to be no missing values from the data frame

###Variables

* Dates: 
    * Appear to be monthly, specifically the first day of each month
    * First Date: January 1, 1978
    * Last Date: February 1, 2012
* Production:
    * Mean: 210.01 (standard deviation of 41.88)
    * Min: 119.41
    * Max: 283.25
* Price:
    * Mean: 2.39 (stdev of 0.7)
    * Min: 1.33
    * Max: 4.43

###Stationarity

* The price series is not stationary, as the time series has strong persistency and inconsistent variation.  In particular, the time series before 2005 shows a strong persistence while the time series after 2005 shows much greater variation.  Also, ACF does not seem to drop off.  The PACF seems to drop off after 2 lags, and the second lag seems to have a negative coefficient.  There are no sudden spikes in either ACF or PACF that would suggest a seasonal effect.
* The histogram of values shows a non-normal distribution with the largest number of the values between 1.5 and 2 with a significant positivie skew.
* The plot of the series shows a large continuous negative shock between 1980 and 1987, a fairly consistent sequence between 1987 and 2005, then a large positive shock after 2005.  
* The fact that this series is not stationary is further supported by the Autocorrelation Function showing significant (albeit decreasing) autocorrelation for all lags displayed.
* The ACF does not seem to drop off.  The PACF seems to drop off after 2 lags, and the second lag seems to have a negative coefficient.  There are no sudden spikes in either ACF or PACF that would suggest a seasonal effect.

####Seasonality

* There appears to be some seasonal variation in the general plot that we will explore further.
* The ACS does show some slight "waves" in the autocorrelations between lags, suggesting possible seasonality.


```{r}
describe(gasOil)
 runEDA = function(x,label,l = 50){
   par(mfrow=c(3,1))
   print(paste("Dickey Fuller Test:",adf.test(x)$p.value)
         )
   # plot time series
   plot.ts(x, main = paste("Time Series Plot of",label),ylab= label)
   
   #plot acf and pacf
   acf(x,main = paste("ACFPlot of",label),ylab = label,lag.max = l)
   pacf(x, main = paste("PACF Plot of",label),ylab = label,lag.max=l)

   #scatter plot matrix
   lag.plot(x, lags=9, layout=c(3,3), 
         diag=TRUE, disg.col="red",
         main=paste("Autocorrelation between ",label," and its Own Lags"))
 }

# run EDA on price
 runEDA(gasOil$Price,"Price")

 # run EDA on production
 runEDA(gasOil$Production,"Production")
```


So let's take the first difference and run EDA again.  The differenced series does not show persistency, but there is still increased variance after 2005.  This is unlikely to be fixed by taking more differences, because the ADF test says that the series does not have any roots greater than |1|.  So I'm going to start the model building process here.

Note that in the differenced series the ACF drops off after 1 lag, but seems to reappear approximately every 6 lags and then lingers for 1 or 2 lags before dropping again.  This suggests a MA component with degree 1 and a seasonal MA component with period 6 and degree 2.  In the PACF graph, the correlation drops off after 2 lags, but then reappears approximately every 5 lags.  This suggest an AR component with degree 2 and seasonal AR component with period 5 and degree 1.

```{r}
diffPrice = diff(gasOil$Price,differences = 1)
runEDA(diffPrice, "Change in Price")
```

#Model Training 

So the general ballpark is SARIMA with order (2,1,1) and seasonal order (1,1,1~2) and period 5~6.  To nail down the exact parameters, we will need to use the AIC:

```{r}
runFitDiagnoseModel = function(x1,o,s,holdout=0){
# 7. Backtesting and Out-of-Sample Forecasting
# Re-estimate the model holding out 10 observations
    original =x1[1:(length(x1)-holdout)] 

  fit <- arima(original, order=o,seasonal = s)


# Model Diagnostics: Residuals
  print(paste("Residual Serial Independence:",Box.test(fit$resid, type="Ljung-Box")$p.value > .05))
  par(mfrow=c(2,2))
  plot(fit$resid, col="blue", main="Residual Series")
  hist(fit$resid, col="gray", main="Residuals")
  acf(fit$resid , main="ACF: Residual Series")
  pacf(fit$resid,main="PACF: Residual Series")

# Model Performance Evaluation: In-Sample Fit

  df <- data.frame(cbind(original,fitted(fit),fit$resid ))
  stargazer(df, type="text", title="Descriptive Stat", digits=1)
##########

# Plot the original and estimate series 
  par(mfrow=c(1,1))
  plot.ts(original, col="navy", 
          main=paste("Original vs ARIMA(",toString(o),"),(",toString(s), ") with Resdiauls"),
          ylab="Original and Estimated Values",
          lty=2,ylim=c(0,4))
  lines(fitted(fit),col="blue",xlab="",ylab="")

  leg.txt <- c("Original", "Estimated", "Residuals")
  legend("bottomleft", legend=leg.txt, lty=c(2,1,1), col=c("navy","blue","green"),
         bty='n', cex=1)

  lines(fit$resid,xlab="",ylab="",col="green",
          pch=1)
  mtext("Residuals", side=4, line=2,col="green")
  #--------------------------------------
if(holdout > 0){
  fit.fcast <- forecast.Arima(fit, h=holdout)

  par(mfrow=c(1,1),new=F)
  plot(fit.fcast,lty=0,
       main="Out-of-Sample Forecast",
     
       ylab="Original, Estimated, and Forecast Values")
  lines(fitted(fit), col="blue")
  lines(x1, col="navy", lty=2)
  leg.txt <- c("Original", "Fitted", "Forecast")
  legend("bottomleft", legend=leg.txt, lty=c(2,1,1),
         col=c("navy","blue","blue"), lwd=c(1,1,2),
         bty='n', cex=1)
  MSE = mean((fit.fcast$mean - tail(x1,holdout))^2)
  print(paste("MSE:",MSE))
}
return(fit)
}
```


Out of the four possible models, model2 [SARIMA (2,1,1),(1,1,1) period 6] has the lowest mean squared error using a holdout of 24.  Also, model1 [SARIMA (2,1,1),(1,1,1) period 5] has a lower AIC than model3 [SARIMA (2,1,1),(1,1,2) period 5].  Model4 [SARIMA (2,1,1),(1,1,2) period 6] returned an error and would not compile.

```{r}
model1=runFitDiagnoseModel(gasOil$Price,c(2,1,1),list(order = c(1,1,1), period=5),24)
```
```{r}
model2=runFitDiagnoseModel(gasOil$Price,c(2,1,1),list(order = c(1,1,1), period=6),24)

```

```{r}
model3=runFitDiagnoseModel(gasOil$Price,c(2,1,1),list(order = c(1,1,2), period=5),24)

```

Model 4 (SARIMA (2,1,1),(1,1,2) period 6) returned an error.  

```{r}
print(try({
model4=runFitDiagnoseModel(gasOil$Price,c(2,1,1),list(order = c(1,1,2), period=6),24)},silent=T
))
```

Further diagnostics show that the 2nd AR and first MA coefficients in non-seasonal component of model2 are not siginificantly different from 0.  This suggests trying to fit a SARIMA (1,1,0), (1,1,1) period 6 model.

```{r}
t( confint(model2) )

```
While the new model5 [SARIMA (1,1,0),(1,1,1)] has some improvment in the MSE, its AIC is higher than model2.  So we're going to stick with model2 for now.

```{r}
model5=runFitDiagnoseModel(gasOil$Price,c(1,1,0),list(order = c(1,1,1), period=6),24)
AIC(model2,model5)
```

Interestingly, an SARIMA (1,1,1),(1,1,3) period 6 is a better fit in terms of both AIC and MSE.  However, the AR signature discovered during EDA suggests an AR(2) series, since the PACF on the differenced series clearly shows correlation in lag 2.  So I'm still going to stick with model2

```{r}
model6=runFitDiagnoseModel(gasOil$Price,c(1,1,1),list(order = c(1,1,3), period=6),24)
AIC(model2,model6)
```

The last step is to retrain the model on the entire data set and then forecast 2 years out.  As you can see, the SARIMA model predicts an continuing increasse in oil prices.

```{r}
modelSarima=runFitDiagnoseModel(gasOil$Price,c(2,1,1),list(order = c(1,1,1), period=6))
  fit.fcast  = forecast.Arima(modelSarima, h=24)

  par(mfrow=c(1,1),new=F)
  plot(fit.fcast,lty=2,
       main="Two Year Forecast",ylim=c(0,5.5),
     
       ylab="Original, Estimated, and Forecast Values",xlim=c(0,435))
  lines(gasOil$Price, col="navy", lty=2)
  leg.txt <- c("Original", "Forecast")
  legend("bottomleft", legend=leg.txt, lty=c(2,1),
         col=c("navy","blue","blue"), lwd=1,
         bty='n', cex=1)


```


Task 2: Create a *multivariate* time series model that can be used to predict/forecast inflation-adjusted gas prices, and use this model to forecast the inflation-adjusted gas price for the next two years
First we will create two fresh timeseries objects, one for the price variable, the other for the production variable. We then conduct some EDA on the two variables. The mean for price is 2.391, while the median for price is 2.096. The price max is 4.432 and the min is 1.329. Production's mean is 210 (median is 201.4), and the min is 119.4 and max is 283.2.

```{r}
# Fitting a linear trend to the Gas Price Data
summary(gasOil)
gasOil$Date = as.Date(gasOil$Date)

price.ts <- ts(gasOil$Price, start=1978, freq=12) 
production.ts <- ts(gasOil$Production, start=1978, freq=12) 
length((price.ts))
length((production.ts))

price.ts.training=ts(price.ts[1:396], start=1978, freq=12) 
price.ts.test=ts(price.ts[397:410], start=2011, freq=12) 
str(price.ts.training)
str(price.ts.test)

production.ts.training=ts(production.ts[1:396], start=1978, freq=12) 
production.ts.test=ts(production.ts[397:410], start=2011, freq=12) 
str(production.ts.training)
str(production.ts.test)

# Scatterplot Matrix
splom(cbind(gasOil$Price,gasOil$Production), 
      main = "Pairwise Scatterplot of Price and Production")

```

Next we analyze how each variable regresses on time. The model regressing price on time does not have a significant p-value (0.3675), and the R-squared is very small (0.002). Neither the coefficient nor intercept shows any signs of statistical significance. A model regressing production onto time does, however, shows high significance with a p-value of 2.2e-16. Both the Intercept and the coefficient are highly significant, and the R-squared is quite high at 0.8937. We take from this regression exercise that the gas price regression model is flat with poor overall fit, while the production model is a much tighter fit and has a nice slope to the regression line. In summary, production appears to have a tighter relationship with time, when compared to price.

```{r}
# Regress Price on time
summary(price.fit_lm <- lm(price.ts.training ~ time(price.ts.training)))
price.fit_lm$coeff
price.fit_lm$qr
head(price.fit_lm$fitted.values)

# Fitting a linear trend to the Gas Production Data
# Regress Production on time
summary(production.fit_lm <- lm(production.ts.training ~ time(production.ts.training)))
production.fit_lm$coeff
production.fit_lm$qr
head(production.fit_lm$fitted.values)

#Plot the regression lines for both Price and Production
par(mfrow=c(2,1))
plot(price.ts.training, col="blue", main="", xlab="Year", ylab="Gas Price")
title("Inflation-Adjusted Gas Prices")
abline(price.fit_lm) # add the fitted regression line to the plot

plot(production.ts.training, col="blue", main="", xlab="Year", ylab="Gas Production")
title("Inflation-Adjusted Gas Production")
abline(production.fit_lm) # add the fitted regression line to the plot

#Look at the first few records for both price and production
head(cbind(price.ts.training, production.ts.training))

```

Then we evaluate the correlation between the price and production variables. The correlation is quite low at 0.0278. We then plot the two lines together and plotting all data points to see if we can visually identify any patterns.  There appears to be no/low correlation between the two variables. The Price vs. Production plot also reveals no discernible patterns.

```{r}

#Evaluate Correlation between price and correlation
cor(price.ts.training, production.ts.training)

#Plot the price and production time series
par(mfrow=c(2,2))
plot.ts(price.ts.training); title("Time-Series Plot of Price")
plot.ts(production.ts.training); title("Time-Series Plot of Production")
ts.plot(ts(price.ts.training),ts(production.ts.training)); title("Time-Series Plot of Price and Production")
plot(price.ts.training,production.ts.training); title("Price vs Production")

# Show summary statistics, even though they might not be useful for a timeseries
summary(cbind(price.ts.training, production.ts.training))

```

Next we plot the autocorrelation and partial autocorrelation functions for both price and production for 36 lags. The ACF of Price shows strong autocorrelation through all lags with some waviness that might represent seasonality. The partial autocorrelation for Price shows the first lag is negatively significant, with the remaining non-significant lags showing an irreguular oscillation around the zero line. The ACF for Production shows even stronger autocorrelation between lags than Price, with an even slower descent as we move out lags. The partial autocorrelation function for Production shows the first 5 lags as significant (although lag 3 was not significant), then lags 11 and 12 are also significant, as were lags 23 and 24. Neither price nor production has a normal distribution of values when we analyze a histogram for each.


```{r}

#Plot the autocorrelation fuction and partial autocorrelation function for both price and production for 36 lags
par(mfrow=c(2,2))
acf(price.ts.training, main="", 36); title("ACF of Price")  
acf(production.ts.training, main="", 36); title("ACF of Production")
pacf(price.ts.training, main="", 36); title("PACF of Price")
pacf(production.ts.training, main="", 36); title("PACF of Production")

#More EDA on Price time series (plot, histogram, acf, and pacf)
par(mfrow=c(2,2))
plot.ts(price.ts.training)
hist(price.ts.training)
acf(price.ts.training)
pacf(price.ts.training)

```

We will then run the Augmented Dickey-Fuller Test to determine if Unit Root is present for both variables. Neither test fail to reject the null hypothesis that each of these series is unit root. We also ran the Phillips-Ouliaris Cointegration test and determined that the variables we could not reject the null hypothesis that the data is not cointegrated.

```{r}

#More EDA on Production time series (plot, histogram, acf, and pacf)
par(mfrow=c(2,2))
plot.ts(production.ts.training)
hist(production.ts.training)
acf(production.ts.training)
pacf(production.ts.training)

#Run the Augmented Dickey-Fuller Test to determine if Unit Root is present for price and production
adf.test(price.ts.training)
adf.test(production.ts.training)
#none of these tests fail to reject the null hypothesis that each of these series is unit root.

#Phillips-Ouliaris Cointegration Test
po.test(cbind(price.ts.training,production.ts.training))

```

We then used the aggregate function to average the data for each year. We then re-plotted the aggregated values to try to identify any patterns.  There were no discernable patterns here either.

```{r}

#Aggregate into an annualized series
cbind(price.ts.training[1:12],production.ts.training[1:12])
cbind(aggregate(price.ts.training), aggregate(production.ts.training))
cbind(aggregate(price.ts.training), aggregate(production.ts.training))
cbind(sum(price.ts.training[1:12]),sum(production.ts.training[1:12]))

tempx<-aggregate(price.ts.training); tempy<-aggregate(production.ts.training)
cbind(tempx[1],tempy[1]) # which is the same as sum(price.ts[1:12]) and sum(production.ts[1:12])

#Plot Aggregate annualized values for Price and Production
par(mfrow=c(1,1))
plot(as.vector(tempx), as.vector(tempy), xlab="Price", ylab="Production", col="blue")
title("Annual Price vs. Annual Production")

#remove objects
rm(tempx, tempy)

# Compute the correlation between the 2 series
cat("Corr(Price , Production): ", cor(price.ts.training,production.ts.training))

```

We will then look at how price regresses on production. The overall model (as well as the production coefficient) show no significance with a p-value of 0.57. The slope intercept is highly significant, but along with the model's non-significant p, and a low R-squared value, there appears to be no linear relationship between these variables. This matches what we witnessed in previous plots of the complete data as well as the aggregate values. An evaluation of the residuals from this model shows strong autocorrelations between lags in the ACF and a non-normal distribution with a strong positive skew.

A cross-correlation of the two time-series shows the lagged relationship between price and production. The peak positive correlation between the two series is at the -2 lag, and likely increases as you move into further negative lags. The correlations decrease as you approach zero lags, and become negative correlations once you move past the positive 2 lag.

```{r}

#Investigating the cointegrated model

# 1. Fit a linear regression model
GAS.lm <- lm(price.ts.training ~ production.ts.training)
summary(GAS.lm)
# The slop coefficient estimate is highly significant, but the overall model and the coefficient are not signficant. # The R-square is very low.

# 2. Obtain the residuals
GAS.res <- resid(GAS.lm)
summary(GAS.res)  
plot(GAS.res, xlab="t", ylab="Residuals", lty=1, pch=1, col="blue")
title("Residuals from the Linear Regression of Price to Production ")

par(mfrow=c(2,2))
plot(GAS.res, xlab="t", ylab="Residuals", lty=1, pch=1, col="blue"); title("Residuals of Price on Production")
plot(density(GAS.res), main="Kernel Density of Residuals")
acf(GAS.res, main="ACF of Residuals")
pacf(GAS.res, main="PACF of Residuals")
# these indicate that residuals still have some persistence

#Cross correlation Function between Price and Production
par(mfrow=c(1,1))
ccf(price.ts.training, production.ts.training, main="Cross-correlation between Price and Production")
# This shows that histogram and correlation may not be effective tools for time series data
pm = ggpairs(cbind(price.ts.training, production.ts.training))
print(pm)

```

And finally we begin to fit models to the two timeseries. We will begin by fitting an autoregressive model to both timeseries (which we bind together). The order of the AR model is 26, suggesting that the best fitting VAR model will also be of order 26. The residuals of this AR(26) model appear to have a normal distribution. A plot of estimated residuals from the AR model resembles white noise except for several periods after 2005 where extreme positive and negative residual values are seen. The autocorrelation function for the residuals of the AR(26) model for price show no correllations at any lag. The ACF for production model residuals show slight negative significance at what appear to be lags 12 and 24.

```{r}

#Fit an Autoregressive Model to both price and production series
GAS.ar <- ar(cbind(price.ts.training, production.ts.training), method="ols", dmean=T, intercept=F)
summary(GAS.ar) # Objects of the estimation results
GAS.ar$ar
GAS.ar$order
#==> it suggests that the best fitting VAR model is of order 26

# Diagnosis using the estimated residuals
dim(GAS.ar$res)
summary(GAS.ar$res)
GAS.ar$res #list the residual

ggpairs(GAS.ar$res) # the residuals do look "fairly" normal and
                   # not correlated with each other

ts.plot(GAS.ar$res[,1], GAS.ar$res[,2], gpars=list(xlab="Simulated Time Period", ylab="Series Values",      lty=c(1:2), pch=c(1,4), col=c("blue","black")))
title("Estimated Resduals from the VAR(26) Model")
leg.txt <- c('Price', 'Production')
legend('topleft', leg.txt, lty=1, col=c('blue', 'black'), bty='n', cex=.75)

```

Next we will fit a vector autoregressive model to the bound price and production time series. The VAR model with period 3 has roots of the characteristic polynomial do not exceed unity in absolute value, raising doubts about the stationarity of this model. I tested VAR models with the period argument of 1 and of 3, neither of which returned polynomial roots that exceeded unity. 

The VAR model must be run on stationary timeseries, so we used the difference operation to help ensure stationarity. When applying a differences of 1, the plot of the resulting model appears to be closer to (but not actually) stationary than the raw time series. However, stationarity is still called into question because the variance is not constant as it becomes more erratic in the later third of the fitted data and the residuals. Because both the raw and the diff'd time series were not stationary, for the rest of this lab we simply used the raw data for simplicity.

We will then use this VAR model to predict gas prices for the subsequent 2 years. The value of the period argument used for the previous VAR model has a large impact on the prediction values.

Using the VAR model with the period = 3, we see a gradual decline in the prediction for price, and a gradual increase in the production levels for the forward-looking 2 years. With a VAR model using a period of 1, the model prediction shows price remaining fairly flat while production increases significantly over the 2 year prediction period. The VAR model shows an order of 28 has the lowest AIC.

```{r}

#Fit a Vector AR Model
#GAS.var <- VAR(cbind(price.ts,production.ts), p=3, type="trend")
x <- cbind(price.ts.training,production.ts.training)
GAS.var <- VAR(x, p=1, type="trend")
summary(GAS.var)
coef(GAS.var)

# 24-Step ahead forecast or 1-year forecast
GAS.pred <- predict(GAS.var, n.ahead=24)

#Predicted values
GAS.pred

#Actual values
production.ts.test

price.pred <- ts(GAS.pred$fcst$price.ts.training[,1], st=c(2011,2,1), fr=12)
production.pred  <- ts(GAS.pred$fcst$production.ts.training[,1], st=c(2011,2,1), fr=12)

#Plot without CI info
ts.plot(cbind(window(price.ts.training, start=1978), price.pred), lty=1:2 )
ts.plot(cbind(window(production.ts.training, start=1978), production.pred), lty=1:2)

VARselect(x, lag.max=30, type="both")
summary(fit <- VAR(x, p=2, type="both"))

summary(VAR(x, p=1, type="both"))  # "both" fits constant + trend

#Run an Autocorrelation on the residuals
acf(resid(fit), 52)
serial.test(fit, lags.pt=12, type="PT.adjusted")

#Plot with 95% CI
fit.pr = predict(fit, n.ahead = 24, ci = 0.95)  # 2 years ahead
fanchart(fit.pr)  # plot prediction + error

```


Task 3: Compare the accuracy of the two models' forecasting results. Also, compare and contrast the results of these two models. Is one model better than the other? What metric(s) do you use to measure whether one model is "better" than the other? Why or why not? Explain the pros and cons of each of the models in this specific context.

The AIC score of SARIMA model is much lower than VAR. By looking at the predicted (backtesting) values, the seasonal arima (SARIMA) model seems to do a better job at predicting the values as the vector auto-regressive shows larger variance for predicted values. The seasonal model captures the pattern on changes every 6 periods, so it is able to capture the seasonal component in the time series pattern better than the VAR. But there is one inherent problem with time-series - even if we can capture seasonality, we still cannot predict new phenomenon that may change the future behavior.


```{r}
#run AIC
AIC(GAS.var,modelSarima)

#merging predicted values and actual values to see them side by side
pred.price=cbind(GAS.pred$fcst$price.ts.training,price.ts.test)
pred.production=cbind(GAS.pred$fcst$production.ts.training,production.ts.test)

# converting forecasted values to ts
GAS.price.pred <- ts(GAS.pred$fcst$price.ts.training[,1], st=2011, fr=12)
GAS.prod.pred  <- ts(GAS.pred$fcst$production.ts.training[,1], st=2011, fr=12)

#below are plots for predicted vs actual values (using VAR)
ts.plot(cbind(window(price.ts, start = 1978), GAS.price.pred), lty = 1:2)
ts.plot(cbind(window(production.ts, start = 1978), GAS.prod.pred), lty = 1:2)

### predicting SARIMA using the final model for 14 steps ahead
modelSarima=runFitDiagnoseModel(price.ts.training,c(2,1,1),list(order = c(1,1,1), period=6))
forecastSarima = forecast.Arima(modelSarima, h=14)

# Price - Comparison of actual vs predicted values (VAR) VS predicted SARIMA
cbind(pred.price[,1],pred.price[,5],forecastSarima$mean[1:14])
pred.price

#Comparison of actual vs predicted values (Production)
cbind(pred.production[,1],pred.production[,5])

# actual vs fitted (Production)
plot.ts(pred.production[,5],type="l",col="red")
lines(pred.production[,1],col="green")
legend('topleft', leg.txt, lty=1, col=c('red', 'green'), bty='n', cex=.75)

# running SARIMA forecase
forecastSarima.price.ts<-ts(forecastSarima$mean[1:14], st=2011, fr=12)
forecastSarima.price.ts
#below are plots for predicted vs actual values (using SARIMA)
ts.plot(cbind(window(price.ts, start = 1978), forecastSarima.price.ts), lty = 1:2,col=c(rep("dark green",1),rep("red",3)))
ts.plot(cbind(window(production.ts, start = 1978), pred.production[,1]), lty = 1:2,col=c(rep("dark green",1),rep("red",3)))

# Examine the residuals:
  head(modelSarima$resid)
  par(mfrow=c(2,2))
  plot(modelSarima, main="Simulated AR1 Model t-plot")
  plot(modelSarima$resid[-1], type="l", main="Residuals t-plot")
  acf(modelSarima$resid[-1], main="ACF of the Residual Series")
  pacf(modelSarima$resid[-1], main="ACF of the Residual Series")

  par(mfrow=c(1,1))
  hist(modelSarima$resid[-1], breaks="FD", col="blue",
       main="Residual Series", ylim=c(0,100))
  qqnorm(modelSarima$resid[-1], main="Normal Q-Q Plot of the Residuals",
         type="p");
  qqline(modelSarima$resid[-1], col="blue")


```
